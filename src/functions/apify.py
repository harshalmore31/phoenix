from apify_client import ApifyClient
import asyncio
import os
import requests
import json
from dotenv import load_dotenv
from rich.console import Console
import google.generativeai as genai

console = Console()
load_dotenv()

def search(query: str) -> str:
    """
    Performs a web search, scrapes content from the top 5 results, and summarizes the content using Gemini.
    This function is synchronous and suitable for use with assistant frameworks.

    Args:
        query: The search query string.

    Returns:
        The summary generated by Gemini as a string.
    """

    print(query)

    async def async_search(query: str) -> str:
        """
        Asynchronous function to perform the search, extraction and summarization.
        """
        # Configuration
        os.makedirs("outputs", exist_ok=True)
        api_key = os.getenv("GEMINI_API_KEY")
        google_api_key = os.getenv("GOOGLE_API_KEY")
        google_cx = os.getenv("GOOGLE_CX")

        def internet_search(search: str) -> list:
            """
            Fetches search results from the Google Custom Search API.
            :param search: Query to search for
            :return: List of result links
            """
            print("Browsing through Google Search!")
            complete_url = f"https://www.googleapis.com/customsearch/v1?key={google_api_key}&cx={google_cx}&q={search}"
            try:
                response = requests.get(complete_url)
                response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
                data = response.json()

                # Print debug information
                total_results = data.get("searchInformation", {}).get("totalResults", "0")
                print(f"Total Results: {total_results}")

                if total_results != "0":
                    results = [{"title": item.get("title"), "link": item.get("link")} for item in data.get("items", []) if
                               "link" in item]

                    # Save search results to a JSON file
                    with open("outputs/search_results.json", "w", encoding="utf-8") as file:
                        json.dump(results, file, indent=4, ensure_ascii=False)

                    print("Search results saved to outputs/search_results.json")
                    return [result["link"] for result in results]
                else:
                    print("No results found")
                    return []
            except requests.exceptions.RequestException as e:
                print(f"Error fetching search results: {e}")
                return []

        # Fetch search results
        search_results = internet_search(query)

        # Limit to top 5 filtered URLs
        filtered_urls = search_results[:5]
        console.print(f"Filtered URLs: {filtered_urls}")

        # Initialize the ApifyClient with your API token
        client = ApifyClient(os.getenv("APIFY_API_TOKEN"))

        # Prepare the Actor input
        run_input = {
            "startUrls": [{"url": url} for url in filtered_urls],
            "useSitemaps": False, # Disable sitemap crawling
            "crawlerType": "playwright:chrome", #Use faster browser without adaptive mode
            "includeUrlGlobs": [],
            "excludeUrlGlobs": [],
            "keepUrlFragments": False,
            "ignoreCanonicalUrl": False,
            "maxCrawlDepth": 0, # Limit crawl depth to 0 to prevent going deeper than the initial URL
            "maxCrawlPages": 1, # Limit to only 1 page per URL
            "maxConcurrency": 5, # Increase concurrency to fetch pages faster
            "proxyConfiguration": {"useApifyProxy": True},
            "removeElementsCssSelector": """
                nav, footer, script, style, noscript, svg, img[src^='data:'],
                [role="alert"], [role="banner"], [role="dialog"], [role="alertdialog"],
                [role="region"][aria-label*="skip" i], [aria-modal="true"]
            """,
            "saveMarkdown": True,
            "maxResults": 5, # We only want top 5 results anyway
        }

        # Run the Actor and wait for it to finish
        run = client.actor("apify/website-content-crawler").call(run_input=run_input)

        # Fetch and aggregate results
        results = []
        for item in client.dataset(run["defaultDatasetId"]).iterate_items():
            results.append(item)

        # Save the aggregated content to a file
        with open("outputs/crawled_content.json", "w", encoding="utf-8") as file:
            json.dump(results, file, indent=4, ensure_ascii=False)

        console.print("Crawled content saved to [bold yellow]outputs/crawled_content.json[/bold yellow]")

        
        api_key = os.getenv("GEMINI_API_KEY")
        genai.configure(api_key=api_key)

        # Create the model
        generation_config = {
            "temperature": 0.4,
            "top_p": 1,
            "top_k": 32,
            "max_output_tokens": 4096,
        }

        # System Instructions for Gemini
        system_instructions = f"""
You are a summarization assistant. Analyze and synthesize information from web search results and extracted content to generate accurate, concise, and comprehensive summaries tailored to the user's query.

**Your Objectives:**

1. **Understand the Query:**
    - Address the user's `{query}` by summarizing essential facts and insights from the retrieved and processed content.
    - Ensure that the summary is relevant and excludes unrelated or repetitive details.

2. **Information Analysis and Summarization:**
    - **Extract Key Information:**
        - Identify central themes, significant facts, arguments, and unique insights from the processed content.
        - Filter out low-quality, irrelevant, or redundant content.
    - **Synthesize Insights:**
        - Combine information from multiple sources into a single, coherent summary.

3. **Output Guidelines:**
    - **Structured Summary:**
        - Deliver the summary in a professional and well-organized markdown format.
        - Use appropriate headings, paragraphs, bullet points, or numbered lists for readability.

4. **Style and Tone:**
    - Use a formal, professional tone suitable for informative summaries.
    - Avoid conversational filler or subjective opinions.
    - Ensure the output is concise, fact-based, and directly answers the user's query.

5. **Content Integration and Optimization:**
    - Prioritize insights from high-quality and reliable sources (e.g., top search results, filtered URLs).
    - Integrate diverse perspectives to provide a balanced and comprehensive view if applicable.

"""

        # Send prompt to Gemini with the extracted content
        model = genai.GenerativeModel(
            model_name="gemini-1.0-pro",
            generation_config=generation_config,

        )

        documents = [result.get("text", "") for result in results]
        
        prompt_parts = [
            system_instructions,
            f"**Query:** {query}\n\n",
            "**Content:**\n" + "\n".join(documents)
        ]

        responsex = model.generate_content(prompt_parts)
        console.print(f"[bold red]{responsex.text}[/bold red]")
        return responsex.text
    

    return asyncio.run(async_search(query))

user_input = input(" Query : ")
search(user_input)